{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics as tm\n",
    "\n",
    "# Device configuration\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "# print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# # Storing ID of current CUDA device\n",
    "# cuda_id = torch.cuda.current_device()\n",
    "# print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "       \n",
    "# print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, g_input_dim, g_output_dim):\n",
    "        super().__init__()       \n",
    "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        return torch.tanh(self.fc4(x))\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        return torch.sigmoid(self.fc4(x))\n",
    "    \n",
    "def D_train(x):\n",
    "    #=======================Train the discriminator=======================#\n",
    "\n",
    "    D.zero_grad()\n",
    "\n",
    "    # train discriminator on real\n",
    "    x_real, y_real = x.view(-1, mnist_dim), torch.ones(bs, 1)\n",
    "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
    "\n",
    "    D_output = D(x_real)\n",
    "    D_real_loss = criterion(D_output, y_real)\n",
    "    D_real_pred = D_output\n",
    "    D_real_target = torch.ones_like(D_real_pred)\n",
    "\n",
    "    # train discriminator on fake\n",
    "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
    "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device))\n",
    "\n",
    "    D_output = D(x_fake)\n",
    "    D_fake_loss = criterion(D_output, y_fake)\n",
    "    D_fake_pred = D_output\n",
    "    D_fake_target = torch.zeros_like(D_fake_pred)\n",
    "\n",
    "    # evaluation metrics\n",
    "    D_realacc.update(D_real_pred, D_real_target)\n",
    "    D_fakeacc.update(D_fake_pred, D_fake_target)\n",
    "    D_accuracy.update(torch.cat((D_real_pred, D_fake_pred), -1), torch.cat((D_real_target, D_fake_target), -1))\n",
    "    G_accuracy.update(D_fake_pred, D_real_target)\n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    D_loss = (D_real_loss + D_fake_loss)\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "        \n",
    "    return  D_loss.item(), D_real_loss.item(), D_fake_loss.item()\n",
    "\n",
    "def G_train(x):\n",
    "    #=======================Train the generator=======================#\n",
    "    G.zero_grad()\n",
    "\n",
    "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
    "    y = Variable(torch.ones(bs, 1).to(device))\n",
    "\n",
    "    G_output = G(z)\n",
    "    D_output = D(G_output)\n",
    "    # Generator loss is the BCE loss of the discriminator output (whether image is real or generated) against labels (set to all real images)\n",
    "    # This value is 0 when the discriminator thinks all generated images are real. So this value should trend to 0. \n",
    "    G_loss = criterion(D_output, y)  \n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "        \n",
    "    return G_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "bs = 1000\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "if torch.cuda.is_available():\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=6)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=10000, shuffle=True, num_workers=6)\n",
    "    #test_loader2 = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=10000, shuffle=True, num_workers=0)\n",
    "    #test_loader2 = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D moved to GPU and using data parallelism\n",
      "G moved to GPU and using data parallelism\n",
      "[1/500]: loss_d: 1.075, loss_g: 0.665\n",
      "[2/500]: loss_d: 1.071, loss_g: 0.458\n",
      "[3/500]: loss_d: 1.241, loss_g: 0.508\n",
      "[4/500]: loss_d: 1.136, loss_g: 0.950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32ma:\\GitRepository\\pytorch-mnist-GAN\\pytorch-mnist-GAN.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X43sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m D_batch_losses, G_batch_losses \u001b[39m=\u001b[39m [], []\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X43sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m D_batch_rlosses, D_batch_flosses \u001b[39m=\u001b[39m [], []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X43sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader2):\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X43sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     total_loss, real_loss, fake_loss \u001b[39m=\u001b[39m D_train(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X43sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     D_batch_losses\u001b[39m.\u001b[39mappend(total_loss)\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_got_empty_message \u001b[39mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[39m.\u001b[39mPeekNamedPipe(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(wait([\u001b[39mself\u001b[39;49m], timeout))\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[39m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[39m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[39m.\u001b[39;49mkeys(), timeout)\n\u001b[0;32m    880\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[39m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m ov \u001b[39min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[39mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWaitForMultipleObjects(L, \u001b[39mFalse\u001b[39;49;00m, timeout)\n\u001b[0;32m    812\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trial 1 bs = 10000, lr = 0.0002\n",
    "\n",
    "bs = 10000\n",
    "\n",
    "# build network\n",
    "z_dim = 100\n",
    "mnist_dim = train_dataset.data.size(1) * train_dataset.data.size(2)\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "# test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=10000, shuffle=True, num_workers=6)\n",
    "    #test_loader2 = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=10000, shuffle=True, num_workers=0)\n",
    "    #test_loader2 = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False, num_workers=0)\n",
    "\n",
    "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)\n",
    "\n",
    "#Set evaluation metrics\n",
    "\n",
    "D_realacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_fakeacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "G_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002\n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    D.to(device)\n",
    "    D = nn.DataParallel(D)\n",
    "    print(\"D moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")   \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    G.to(device)\n",
    "    G = nn.DataParallel(G)\n",
    "    print(\"G moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")    \n",
    "\n",
    "n_epoch = 500\n",
    "D_losses, G_losses = [], []\n",
    "D_r_acc, D_f_acc = [], []\n",
    "D_r_loss, D_f_loss = [], []\n",
    "D_acc, G_acc = [], []\n",
    "test_z = Variable(torch.randn(104, z_dim).to(device))\n",
    "\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    \n",
    "    D_batch_losses, G_batch_losses = [], []\n",
    "    D_batch_rlosses, D_batch_flosses = [], []\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader2):\n",
    "        total_loss, real_loss, fake_loss = D_train(x)\n",
    "        D_batch_losses.append(total_loss)\n",
    "        D_batch_rlosses.append(real_loss)\n",
    "        D_batch_flosses.append(fake_loss)\n",
    "        G_batch_losses.append(G_train(x))\n",
    "\n",
    "\n",
    "    D_r_acc.append(D_realacc.compute().cpu())\n",
    "    D_f_acc.append(D_fakeacc.compute().cpu())\n",
    "    D_acc.append(D_accuracy.compute().cpu())\n",
    "    G_acc.append(G_accuracy.compute().cpu())\n",
    "\n",
    "    D_epochtotalloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_epochrloss = torch.mean(torch.FloatTensor(D_batch_rlosses))\n",
    "    D_epochfloss = torch.mean(torch.FloatTensor(D_batch_flosses))\n",
    "    G_epochloss = torch.mean(torch.FloatTensor(G_batch_losses))\n",
    "    D_epochloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_losses.append(D_epochtotalloss)\n",
    "    D_r_loss.append(D_epochrloss)\n",
    "    D_f_loss.append(D_epochfloss)\n",
    "    G_losses.append(G_epochloss)\n",
    "\n",
    "    D_realacc.reset()\n",
    "    D_fakeacc.reset()\n",
    "    D_accuracy.reset()\n",
    "    G_accuracy.reset()\n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            generated = G(test_z)\n",
    "            save_image(generated.view(generated.size(0), 1, 28, 28), './mnist samples/Trial 1/sample_{}'.format(epoch) + '.png')\n",
    "\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, D_epochloss, G_epochloss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_loss, label=\"Discriminator Real Loss\")\n",
    "plt.plot(D_f_loss, label=\"Discriminator Fake Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 1/Loss_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses, label=\"Discriminator Total Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 1/Loss_plot2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_acc, label=\"Discriminator Total Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 1/Acc_plot2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_acc, label=\"Discriminator Real Accuracy\")\n",
    "plt.plot(D_f_acc, label=\"Discriminator Fake Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 1/Acc_plot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D moved to GPU and using data parallelism\n",
      "G moved to GPU and using data parallelism\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1000, 1])) that is different to the input size (torch.Size([10000, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32ma:\\GitRepository\\pytorch-mnist-GAN\\pytorch-mnist-GAN.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m D_batch_rlosses, D_batch_flosses \u001b[39m=\u001b[39m [], []\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader2):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     total_loss, real_loss, fake_loss \u001b[39m=\u001b[39m D_train(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     D_batch_losses\u001b[39m.\u001b[39mappend(total_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     D_batch_rlosses\u001b[39m.\u001b[39mappend(real_loss)\n",
      "\u001b[1;32ma:\\GitRepository\\pytorch-mnist-GAN\\pytorch-mnist-GAN.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m x_real, y_real \u001b[39m=\u001b[39m Variable(x_real\u001b[39m.\u001b[39mto(device)), Variable(y_real\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m D_output \u001b[39m=\u001b[39m D(x_real)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m D_real_loss \u001b[39m=\u001b[39m criterion(D_output, y_real)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m D_real_pred \u001b[39m=\u001b[39m D_output\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/GitRepository/pytorch-mnist-GAN/pytorch-mnist-GAN.ipynb#X46sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m D_real_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(D_real_pred)\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32ma:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([1000, 1])) that is different to the input size (torch.Size([10000, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Trial 2 bs = 1000, lr = 0.0002\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bs = 1000\n",
    "\n",
    "# build network\n",
    "z_dim = 100\n",
    "mnist_dim = train_dataset.data.size(1) * train_dataset.data.size(2)\n",
    "\n",
    "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)\n",
    "\n",
    "#Set evaluation metrics\n",
    "\n",
    "D_realacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_fakeacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "G_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "# test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "if torch.cuda.is_available():\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=6)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002\n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    D.to(device)\n",
    "    D = nn.DataParallel(D)\n",
    "    print(\"D moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")   \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    G.to(device)\n",
    "    G = nn.DataParallel(G)\n",
    "    print(\"G moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")    \n",
    "\n",
    "n_epoch = 500\n",
    "D_losses, G_losses = [], []\n",
    "D_r_acc, D_f_acc = [], []\n",
    "D_r_loss, D_f_loss = [], []\n",
    "D_acc, G_acc = [], []\n",
    "test_z = Variable(torch.randn(104, z_dim).to(device))\n",
    "\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    \n",
    "    D_batch_losses, G_batch_losses = [], []\n",
    "    D_batch_rlosses, D_batch_flosses = [], []\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        total_loss, real_loss, fake_loss = D_train(x)\n",
    "        D_batch_losses.append(total_loss)\n",
    "        D_batch_rlosses.append(real_loss)\n",
    "        D_batch_flosses.append(fake_loss)\n",
    "        G_batch_losses.append(G_train(x))\n",
    "\n",
    "\n",
    "    D_r_acc.append(D_realacc.compute().cpu())\n",
    "    D_f_acc.append(D_fakeacc.compute().cpu())\n",
    "    D_acc.append(D_accuracy.compute().cpu())\n",
    "    G_acc.append(G_accuracy.compute().cpu())\n",
    "\n",
    "    D_epochtotalloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_epochrloss = torch.mean(torch.FloatTensor(D_batch_rlosses))\n",
    "    D_epochfloss = torch.mean(torch.FloatTensor(D_batch_flosses))\n",
    "    G_epochloss = torch.mean(torch.FloatTensor(G_batch_losses))\n",
    "    D_epochloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_losses.append(D_epochtotalloss)\n",
    "    D_r_loss.append(D_epochrloss)\n",
    "    D_f_loss.append(D_epochfloss)\n",
    "    G_losses.append(G_epochloss)\n",
    "\n",
    "    D_realacc.reset()\n",
    "    D_fakeacc.reset()\n",
    "    D_accuracy.reset()\n",
    "    G_accuracy.reset()\n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            generated = G(test_z)\n",
    "            save_image(generated.view(generated.size(0), 1, 28, 28), './mnist samples/Trial 2/sample_{}'.format(epoch) + '.png')\n",
    "\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, D_epochloss, G_epochloss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_loss, label=\"Discriminator Real Loss\")\n",
    "plt.plot(D_f_loss, label=\"Discriminator Fake Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 2/Loss_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_acc, label=\"Discriminator Real Accuracy\")\n",
    "plt.plot(D_f_acc, label=\"Discriminator Fake Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 2/Acc_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses, label=\"Discriminator Total Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 2/Loss_plot2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_acc, label=\"Discriminator Total Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 2/Acc_plot2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 3 bs = 1000, G_lr = 0.0003 D_lr = 0.0001\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bs = 1000\n",
    "\n",
    "# build network\n",
    "z_dim = 100\n",
    "mnist_dim = train_dataset.data.size(1) * train_dataset.data.size(2)\n",
    "\n",
    "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)\n",
    "\n",
    "#Set evaluation metrics\n",
    "\n",
    "D_realacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_fakeacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "G_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "# test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "if torch.cuda.is_available():\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=6)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002\n",
    "G_optimizer = optim.Adam(G.parameters(), lr = 0.0003)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = 0.0001)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    D.to(device)\n",
    "    D = nn.DataParallel(D)\n",
    "    print(\"D moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")   \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    G.to(device)\n",
    "    G = nn.DataParallel(G)\n",
    "    print(\"G moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")    \n",
    "\n",
    "n_epoch = 500\n",
    "D_losses, G_losses = [], []\n",
    "D_r_acc, D_f_acc = [], []\n",
    "D_r_loss, D_f_loss = [], []\n",
    "D_acc, G_acc = [], []\n",
    "test_z = Variable(torch.randn(104, z_dim).to(device))\n",
    "\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    \n",
    "    D_batch_losses, G_batch_losses = [], []\n",
    "    D_batch_rlosses, D_batch_flosses = [], []\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        total_loss, real_loss, fake_loss = D_train(x)\n",
    "        D_batch_losses.append(total_loss)\n",
    "        D_batch_rlosses.append(real_loss)\n",
    "        D_batch_flosses.append(fake_loss)\n",
    "        G_batch_losses.append(G_train(x))\n",
    "\n",
    "\n",
    "    D_r_acc.append(D_realacc.compute().cpu())\n",
    "    D_f_acc.append(D_fakeacc.compute().cpu())\n",
    "    D_acc.append(D_accuracy.compute().cpu())\n",
    "    G_acc.append(G_accuracy.compute().cpu())\n",
    "\n",
    "    D_epochtotalloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_epochrloss = torch.mean(torch.FloatTensor(D_batch_rlosses))\n",
    "    D_epochfloss = torch.mean(torch.FloatTensor(D_batch_flosses))\n",
    "    G_epochloss = torch.mean(torch.FloatTensor(G_batch_losses))\n",
    "    D_epochloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_losses.append(D_epochtotalloss)\n",
    "    D_r_loss.append(D_epochrloss)\n",
    "    D_f_loss.append(D_epochfloss)\n",
    "    G_losses.append(G_epochloss)\n",
    "\n",
    "    D_realacc.reset()\n",
    "    D_fakeacc.reset()\n",
    "    D_accuracy.reset()\n",
    "    G_accuracy.reset()\n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            generated = G(test_z)\n",
    "            save_image(generated.view(generated.size(0), 1, 28, 28), './mnist samples/Trial 3/sample_{}'.format(epoch) + '.png')\n",
    "\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, D_epochloss, G_epochloss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_loss, label=\"Discriminator Real Loss\")\n",
    "plt.plot(D_f_loss, label=\"Discriminator Fake Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 3/Loss_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_acc, label=\"Discriminator Real Accuracy\")\n",
    "plt.plot(D_f_acc, label=\"Discriminator Fake Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 3/Acc_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses, label=\"Discriminator Total Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 3/Loss_plot2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_acc, label=\"Discriminator Total Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 3/Acc_plot2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 4 bs = 1000, lr = 0.0001\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bs = 1000\n",
    "\n",
    "# build network\n",
    "z_dim = 100\n",
    "mnist_dim = train_dataset.data.size(1) * train_dataset.data.size(2)\n",
    "\n",
    "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)\n",
    "\n",
    "#Set evaluation metrics\n",
    "\n",
    "D_realacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_fakeacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "G_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "# test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "if torch.cuda.is_available():\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=6)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0001\n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    D.to(device)\n",
    "    D = nn.DataParallel(D)\n",
    "    print(\"D moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")   \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    G.to(device)\n",
    "    G = nn.DataParallel(G)\n",
    "    print(\"G moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")    \n",
    "\n",
    "n_epoch = 500\n",
    "D_losses, G_losses = [], []\n",
    "D_r_acc, D_f_acc = [], []\n",
    "D_r_loss, D_f_loss = [], []\n",
    "D_acc, G_acc = [], []\n",
    "test_z = Variable(torch.randn(104, z_dim).to(device))\n",
    "\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    \n",
    "    D_batch_losses, G_batch_losses = [], []\n",
    "    D_batch_rlosses, D_batch_flosses = [], []\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        total_loss, real_loss, fake_loss = D_train(x)\n",
    "        D_batch_losses.append(total_loss)\n",
    "        D_batch_rlosses.append(real_loss)\n",
    "        D_batch_flosses.append(fake_loss)\n",
    "        G_batch_losses.append(G_train(x))\n",
    "\n",
    "\n",
    "    D_r_acc.append(D_realacc.compute().cpu())\n",
    "    D_f_acc.append(D_fakeacc.compute().cpu())\n",
    "    D_acc.append(D_accuracy.compute().cpu())\n",
    "    G_acc.append(G_accuracy.compute().cpu())\n",
    "\n",
    "    D_epochtotalloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_epochrloss = torch.mean(torch.FloatTensor(D_batch_rlosses))\n",
    "    D_epochfloss = torch.mean(torch.FloatTensor(D_batch_flosses))\n",
    "    G_epochloss = torch.mean(torch.FloatTensor(G_batch_losses))\n",
    "    D_epochloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_losses.append(D_epochtotalloss)\n",
    "    D_r_loss.append(D_epochrloss)\n",
    "    D_f_loss.append(D_epochfloss)\n",
    "    G_losses.append(G_epochloss)\n",
    "\n",
    "    D_realacc.reset()\n",
    "    D_fakeacc.reset()\n",
    "    D_accuracy.reset()\n",
    "    G_accuracy.reset()\n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            generated = G(test_z)\n",
    "            save_image(generated.view(generated.size(0), 1, 28, 28), './mnist samples/Trial 4/sample_{}'.format(epoch) + '.png')\n",
    "\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, D_epochloss, G_epochloss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_loss, label=\"Discriminator Real Loss\")\n",
    "plt.plot(D_f_loss, label=\"Discriminator Fake Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 4/Loss_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_acc, label=\"Discriminator Real Accuracy\")\n",
    "plt.plot(D_f_acc, label=\"Discriminator Fake Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 4/Acc_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses, label=\"Discriminator Total Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 4/Loss_plot2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_acc, label=\"Discriminator Total Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 4/Acc_plot2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 5 bs = 1000, lr = 0.0002 beta1=0.5, beta2=0.9\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bs = 1000\n",
    "\n",
    "# build network\n",
    "z_dim = 100\n",
    "mnist_dim = train_dataset.data.size(1) * train_dataset.data.size(2)\n",
    "\n",
    "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)\n",
    "\n",
    "#Set evaluation metrics\n",
    "\n",
    "D_realacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_fakeacc = tm.classification.BinaryAccuracy().to(device)\n",
    "D_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "G_accuracy = tm.classification.BinaryAccuracy().to(device)\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "# test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "if torch.cuda.is_available():\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=6)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=6)\n",
    "else: \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    #test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=0)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002 \n",
    "beta1 = 0.6\n",
    "beta2 = 0.9\n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr, betas=(beta1, beta2))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr, betas=(beta1, beta2))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    D.to(device)\n",
    "    D = nn.DataParallel(D)\n",
    "    print(\"D moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")   \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    G.to(device)\n",
    "    G = nn.DataParallel(G)\n",
    "    print(\"G moved to GPU and using data parallelism\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")    \n",
    "\n",
    "n_epoch = 500\n",
    "D_losses, G_losses = [], []\n",
    "D_r_acc, D_f_acc = [], []\n",
    "D_r_loss, D_f_loss = [], []\n",
    "D_acc, G_acc = [], []\n",
    "test_z = Variable(torch.randn(104, z_dim).to(device))\n",
    "\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    \n",
    "    D_batch_losses, G_batch_losses = [], []\n",
    "    D_batch_rlosses, D_batch_flosses = [], []\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        total_loss, real_loss, fake_loss = D_train(x)\n",
    "        D_batch_losses.append(total_loss)\n",
    "        D_batch_rlosses.append(real_loss)\n",
    "        D_batch_flosses.append(fake_loss)\n",
    "        G_batch_losses.append(G_train(x))\n",
    "\n",
    "\n",
    "    D_r_acc.append(D_realacc.compute().cpu())\n",
    "    D_f_acc.append(D_fakeacc.compute().cpu())\n",
    "    D_acc.append(D_accuracy.compute().cpu())\n",
    "    G_acc.append(G_accuracy.compute().cpu())\n",
    "\n",
    "    D_epochtotalloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_epochrloss = torch.mean(torch.FloatTensor(D_batch_rlosses))\n",
    "    D_epochfloss = torch.mean(torch.FloatTensor(D_batch_flosses))\n",
    "    G_epochloss = torch.mean(torch.FloatTensor(G_batch_losses))\n",
    "    D_epochloss = torch.mean(torch.FloatTensor(D_batch_losses))\n",
    "    D_losses.append(D_epochtotalloss)\n",
    "    D_r_loss.append(D_epochrloss)\n",
    "    D_f_loss.append(D_epochfloss)\n",
    "    G_losses.append(G_epochloss)\n",
    "\n",
    "    D_realacc.reset()\n",
    "    D_fakeacc.reset()\n",
    "    D_accuracy.reset()\n",
    "    G_accuracy.reset()\n",
    "        \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            generated = G(test_z)\n",
    "            save_image(generated.view(generated.size(0), 1, 28, 28), './mnist samples/Trial 5/sample_{}'.format(epoch) + '.png')\n",
    "\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, D_epochloss, G_epochloss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_loss, label=\"Discriminator Real Loss\")\n",
    "plt.plot(D_f_loss, label=\"Discriminator Fake Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 5/Loss_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_r_acc, label=\"Discriminator Real Accuracy\")\n",
    "plt.plot(D_f_acc, label=\"Discriminator Fake Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 5/Acc_plot.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_losses, label=\"Discriminator Total Loss\")\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('./mnist samples/Trial 5/Loss_plot2.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(D_acc, label=\"Discriminator Total Accuracy\")\n",
    "plt.plot(G_acc, label=\"Generator Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('./mnist samples/Trial 5/Acc_plot2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc1): Linear(in_features=100, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (fc4): Linear(in_features=1024, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
